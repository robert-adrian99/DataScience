{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucur Robert - Adrian\n",
    "# Bogdan Gheorghe - Nicolae\n",
    "## Grupa 10LF381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de regresie\n",
    "\n",
    "Folositi urmatoarele seturi de date:\n",
    "\n",
    "1. [CPU Computer Hardware](https://archive.ics.uci.edu/ml/datasets/Computer+Hardware); excludeti din dataset coloanele: vendor name, model name, estimated relative performance; se va estima coloana \"published relative performance\".\n",
    "1. [Boston Housing](http://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n",
    "1. [Wisconsin Breast Cancer](http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html); cautati in panelul din stanga Wisconsin Breast Cancer si urmati pasii din \"My personal Notes\"\n",
    "1. [Communities and Crime](http://archive.ics.uci.edu/ml/datasets/communities+and+crime); stergeti primele 5 dimensiuni si trasaturile cu missing values.\n",
    "\n",
    "Pentru fiecare set de date aplicati minim 5 modele de regresie din scikit learn. Pentru fiecare raportati: mean absolute error, mean squared error, median absolute error - a se vedea [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) - folosind 5 fold cross validation. Valorile hiperparametrilor trebuie cautate cu grid search (cv=3)  si random search (n_iter dat de voi). Metrica folosita pentru cautarea hiperparametrilor va fi mean squared error. Raportati mediile rezultatelor atat pentru fold-urile de antrenare, cat si pentru cele de testare; indicatie: puteti folosi metoda `cross_validate` cu parametrul `return_train_score=True`, iar ca model un obiect de tip `GridSearchCV` sau `RandomizedSearchCV`.\n",
    "\n",
    "Rezultatele vor fi trecute intr-un dataframe. Intr-o stare intermediara, valorile vor fi calculate cu semnul minus: din motive de implementare, biblioteca sklearn transforma scorurile in numere negative; a se vedea imaginea de mai jos:\n",
    "\n",
    "![intermediate report](./images/cpu_intermediate_blurred.png)\n",
    "\n",
    "\n",
    "Valorile vor fi aduse la interval pozitiv, apoi vor fi marcate cele maxime si minime; orientativ, se poate folosi imaginea de mai jos, reprezentand dataframe afisat in notebook; puteti folosi alte variante de styling pe dataframe precum la https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#.  \n",
    "\n",
    "Se va crea un raport final in format HTML sau PDF - fisier(e) separat(e). Raportul trebuie sa contina minimal: numele setului de date si obiectul dataframe; preferabil sa se pastreze marcajul de culori realizat in notebook.\n",
    "\n",
    "![report](./images/cpu_results_blurred.png)\n",
    "\n",
    "Notare:\n",
    "\n",
    "1. Se acorda 20 de puncte din oficiu.\n",
    "1. Optimizare si cuantificare de performanta a modelelor: 3 puncte pentru fiecare combinatie set de date + model = 60 de puncte\n",
    "1. Documentare modele: numar modele * 2 puncte = 10 puncte. Documentati in jupyter notebook fiecare din modelele folosite, in limba romana. Puteti face o sectiune separata cu documentarea algoritmilor. Fiecare model trebuie sa aiba o descriere de minim 20 de randuri, minim o imagine asociata si minim 2 referinte bibliografice.\n",
    "1. 10 puncte: export in format HTML sau PDF.\n",
    "\n",
    "\n",
    "*Notare:* rezolvarea va fi incarcata pe platforma de elearning in saptamana 11-15 mai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce sunt arborii de decizie?\n",
    "\n",
    "** Un arbore de decizie este un instrument de asistență decizională care folosește un model de decizie asemănător arborelui și consecințele posibile ale acestora, inclusiv rezultatele evenimentului întâmplător, costurile resurselor și utilitatea. Este o modalitate de a afișa un algoritm care conține doar declarații de control condiționate. **\n",
    "** Un arbore de decizie este o structură asemănătoare cu o diagramă de flux în care fiecare nod intern reprezintă un „test” pe un atribut (de exemplu, dacă o aruncare de monedă are cap sau pajura), fiecare ramură reprezintă rezultatul testului și fiecare nod frunză reprezintă o etichetă de clasă (decizia luată după calcularea tuturor atributelor). Căile de la rădăcină până la frunze reprezintă reguli de clasificare. **\n",
    "\n",
    "## Cum se clasifica?\n",
    "\n",
    "** Arborii decizionali sunt împărțiți în arbori de clasificare și regresie. Arborii de regresie sunt necesari atunci când variabila de răspuns este numerică sau continuă. Arborii de clasificare, așa cum sugerează numele, sunt folosiți pentru a separa setul de date în clase aparținând variabilei de răspuns. Această piesă explică o practică de model de regresie a deciziei cu Python. **\n",
    "\n",
    "| ![Decision Tree](./images/DecisionTree.png) | ![Decision Tree Regressor](./images/DecisionTreeRegressor.png) |\n",
    "|:---:|:---:|\n",
    "\n",
    "## Cum functioneaza arborii de decizie?\n",
    "\n",
    "** Arborele decizional construiește modele de regresie sau clasificare sub forma unei structuri de arbori. Acesta descompune un set de date în subseturi mai mici și mai mici, în același timp se dezvoltă treptat un arbore de decizie asociat. Rezultatul final este un arbore cu noduri de decizie și noduri frunze. Un nod de decizie (de exemplu, Outlook) are două sau mai multe ramuri (de exemplu, însorit, înnorat și ploios), fiecare reprezentând valori pentru atributul testat. Nodul frunzei (de exemplu, ore jucate) reprezintă o decizie cu privire la ținta numerică. Cel mai înalt nod de decizie într-un arbore corespunde celui mai bun predictor, numit nod rădăcină. Arborii de decizie pot trata atât date categorice cât și numerice. **\n",
    "\n",
    "** Algoritmul arborelui de deciziei se încadrează în categoria algoritmilor de învățare supravegheată. Funcționează atât pentru variabile de ieșire continue, cât și pentru categorii. Regresia arborelui decizional observă caracteristicile unui obiect și antrenează un model în structura unui arbore pentru a prezice date în viitor, pentru a produce o ieșire continuă semnificativă. Ieșirea continuă înseamnă că ieșirea / rezultatul nu este discretă, adică nu este reprezentată doar de un set discret, cunoscut de numere sau valori. **\n",
    "\n",
    "### Bibliografie:\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "2. https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/\n",
    "3. https://medium.com/pursuitnotes/decision-tree-regression-in-6-steps-with-python-1a1c5aa2ee16\n",
    "4. https://www.saedsayad.com/decision_tree_reg.htm\n",
    "5. https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce este Regresia Liniara?\n",
    "\n",
    "** Regresia liniară este utilizată pentru găsirea unei relații liniare între țintă și unul sau mai mulți predictori. Există două tipuri de regresie liniară - simplă și multiplă. **\n",
    "** Regresia liniară simplă este utilă pentru a găsi relația între două variabile continue. Una este predictoare, sau variabila independentă, iar alta este variabila de răspuns sau dependentă. Se caută relația statistică, dar nu relația deterministă. Se spune că relația dintre două variabile este deterministă dacă o variabilă poate fi exprimată cu exactitate de cealaltă. De exemplu, folosind temperatura în grade Celsius este posibil să prezicem cu exactitate Fahrenheit. Relația statistică nu este exactă pentru a determina relația dintre două variabile. De exemplu, relația dintre înălțime și greutate. **\n",
    "\n",
    "| ![Linear Regression](./images/LinearRegression.png) | ![Linear Regression 3D](./images/LinearRegression3D.png) |\n",
    "|:---:|:---:|\n",
    "\n",
    "** În regresia liniară, relațiile sunt modelate folosind funcții de predictor liniare ai căror parametri de model necunoscuți sunt estimati din date. Astfel de modele sunt numite modele liniare. Cel mai frecvent, se presupune că media condițională a răspunsului dat, valorile variabilelor explicative (sau predictori), este o funcție afină a acelor valori; mai puțin frecvent se folosește mediana condițională sau o alta cuantila. La fel ca toate formele de analiză de regresie, regresia liniară se concentrează pe distribuția probabilității condiționale a răspunsului dat, valorile predictorilor, mai degrabă decât pe distribuția comună a probabilității pentru toate aceste variabile, care este domeniul analizei multivariate. **\n",
    "\n",
    "## Cum functioneaza Regresia Liniara?\n",
    "\n",
    "** Regresia liniară a fost primul tip de analiză de regresie care a fost studiat riguros și care a fost utilizat pe scară largă în aplicații practice. Acest lucru se datorează faptului că modelele care depind liniar de parametrii lor necunoscuți sunt mai ușor de adaptat decât modelele care nu sunt liniar legate de parametrii lor și pentru că proprietățile statistice ale estimatorilor rezultați sunt mai ușor de determinat. ** \n",
    "\n",
    "## Regresia liniara in Machine learning\n",
    "\n",
    "** Machine learning, mai precis domeniul modelării predictive, este în primul rând preocupat de minimizarea erorii unui model sau de a face cele mai precise previziuni posibile, în detrimentul explicabilității. În machine learning vom împrumuta, reutiliza și fura algoritmi din mai multe domenii diferite, inclusiv statistici, și le vom folosi în acest scop.\n",
    "Ca atare, regresia liniară a fost dezvoltată în domeniul statisticilor și este studiată ca model pentru înțelegerea relației dintre variabilele numerice de intrare și de ieșire, dar a fost împrumutată in machine learning. Este atât un algoritm statistic, cât și un algoritm de machine learning. ** \n",
    "\n",
    "### Bibliografie:\n",
    "1. https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86\n",
    "2. https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "3. https://en.wikipedia.org/wiki/Linear_regression\n",
    "4. https://scikit-learn.org/stable/modules/linear_model.html#linear-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** În statistică și învățare automată, lasso (least absolute shrinkage and selection operator; de asemenea, Lasso sau LASSO) este o metodă de analiză a regresiei care realizează atât selecția variabilă, cât și regularizarea, pentru a spori exactitatea și interpretarea predicției modelului statistic pe care îl produce. A fost introdus inițial în literatura de geofizică în 1986, și ulterior redescoperită și popularizată în mod independent în 1996 de Robert Tibshirani, care a inventat termenul și a oferit informații suplimentare despre performanța observată.\n",
    "Lasso a fost introdus pentru a îmbunătăți exactitatea predicției și interpretabilitatea modelelor de regresie prin modificarea procesului de montare a modelului, pentru a selecta doar un subset al covariatelor furnizate pentru a fi utilizate în modelul final, mai degrabă decât pentru a le folosi pe toate. Acesta a fost dezvoltat independent în geofizică, pe baza lucrărilor anterioare care au folosit pedeapsa ℓ 1 {\\ displaystyle \\ ell ^ {1}} \\ ell ^ {1} atât pentru montarea, cât și pentru penalizarea coeficienților, și de către statisticianul, Robert Tibshirani pe garrote non-negative ale lui Breiman.\n",
    "Lasso este capabil să atingă ambele aceste obiective, forțând ca suma valorii absolute a coeficienților de regresie să fie mai mică decât o valoare fixă, ceea ce obligă la determinarea anumitor coeficienți la zero, alegând efectiv un model mai simplu care nu include acei coeficienți. . Această idee este similară cu regresia creastă, în care suma pătratelor coeficienților este forțată să fie mai mică decât o valoare fixă, deși în cazul regresiei creșei, aceasta reduce doar dimensiunea coeficienților, nu stabilește niciunul dintre ei la zero. **\n",
    "\n",
    "## Ce este Lasso Regression?\n",
    "** Regresia Lasso este un tip de regresie liniară care folosește contracția. Reducerea este locul în care valorile datelor sunt reduse spre un punct central ca media. Procedura lasso încurajează modele simple, rare (adică modele cu mai puțini parametri). Acest tip particular de regresie este potrivit pentru modelele care prezintă niveluri ridicate de muticolinearitate sau când doriți să automatizați anumite părți ale selecției modelului, cum ar fi selectarea variabilă / eliminarea parametrilor. **\n",
    "\n",
    "** Regresia Lasso efectuează regularizarea L1, adică adaugă un factor al sumei valorii absolute a coeficienților în obiectivul de optimizare. Astfel, regresia lasso optimizează următoarele:\n",
    "Obiectiv = RSS + α * (suma valorii absolute a coeficienților)**\n",
    "\n",
    "![LassoEx1](./images/LassoEx1-250x250.png)\n",
    "\n",
    "## Specificația modelului:\n",
    "** Singura diferență în funcțiile de creștere și pierdere de nivel este în termeni de penalizare. În baza lasso, pierderea este definită ca:**\n",
    "\n",
    "![LassoEx2](./images/LassoEx2.png)\n",
    "\n",
    "** Atât suma pătratelor, cât și pedeapsa lasso sunt convexe, la fel și funcția de pierdere a lasso-ului. În consecință, există un minim global. Cu toate acestea, funcția de pierdere a lasso-ului nu este strict convexă. În consecință, pot exista mai multe β care reduc la minimum funcția de pierdere a lasso-ului. **\n",
    "\n",
    "** În general, nu există o soluție explicită care să optimizeze funcția de pierdere de lasso. **\n",
    "\n",
    "** Recurgeți la proceduri de optimizare numerică, de exemplu, ascensiune în gradient. **\n",
    "\n",
    "### Bibliografie:\n",
    "1. https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "1. https://www.statisticshowto.com/lasso-regression/\n",
    "1. https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "1. https://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "1. https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "1. http://www.few.vu.nl/~wvanwie/Courses/HighdimensionalDataAnalysis/WNvanWieringen_HDDA_Lecture56_LassoRegression_20182019.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RandomForestRegressor Ex1](./images/RandomForestRegressorEx1-250x250.png)\n",
    "\n",
    "## Cum funcționează RandomForest?\n",
    "\n",
    "** RandomForest este o tehnică de ansamblu capabilă să îndeplinească atât sarcini de regresie, cât și de clasificare, folosind arbori de decizie multiple și o tehnică numită Bootstrap Agregation, cunoscută în mod obișnuit ca bagging. Ce bagaj poți întreba? Bagajul, în metoda Random Forest, implică instruirea fiecărui arbore de decizie pe un eșantion de date diferit, unde eșantionarea se face cu înlocuirea.\n",
    "Ideea de bază din spatele acestui lucru este de a combina mai mulți arbori de decizie în determinarea producției finale, mai degrabă decât să se bazeze pe arbori de decizie individuale. **\n",
    "\n",
    "![RandomForestRegressor Ex1](./images/RandomForestRegressorEx2-250x250.jpeg)\n",
    "\n",
    "## Cum funcționează algoritmul RandomForestRegressor?\n",
    "### Următoarele sunt etapele de bază implicate în realizarea algoritmului RandomForestRegressor:\n",
    "\n",
    "1. ** Alegeți N înregistrări aleatorii din setul de date. **\n",
    "2. ** Construiți un arbore de decizie pe baza acestor N înregistrări. **\n",
    "3. ** Alegeți numărul de arbori pe care doriți în algoritmul dvs. și repetați pașii 1 și 2. **\n",
    "4. ** În cazul unei probleme de regresie, pentru o înregistrare nouă, fiecare copac din pădure prezice o valoare pentru Y (ieșire). Valoarea finală poate fi calculată luând media tuturor valorilor prevăzute de toți arborii din pădure. Sau, în cazul unei probleme de clasificare, fiecare copac din pădure prezice categoria din care face parte noua înregistrare. În cele din urmă, noua înregistrare este atribuită categoriei care câștigă votul majorității. **\n",
    "\n",
    "## Avantaje ale utilizarii Random Forest\n",
    "### Ca în orice algoritm, există avantaje și dezavantaje în utilizarea acestuia.\n",
    "1. ** Algoritmul random forest nu este părtinitor, deoarece, există mai mulți arbori și fiecare arbore este instruit pe un subset de date. Practic, algoritmul rand forest se bazează pe puterea „mulțimii”; prin urmare, prejudecata generală a algoritmului este redusă. **\n",
    "2. ** Acest algoritm este foarte stabil. Chiar dacă un nou punct de date este introdus în setul de date, algoritmul general nu este afectat foarte mult, deoarece datele noi pot avea un impact asupra unui arbore, dar este foarte greu să aibă impact asupra tuturor copacilor. **\n",
    "3. ** Algoritmul forestier aleatoriu funcționează bine atunci când aveți caracteristici atât categorice cât și numerice. **\n",
    "4. ** Algoritmul forestier aleatoriu funcționează bine și atunci când datele au valori lipsă sau nu au fost scalate bine (deși am efectuat scalarea caracteristicilor în acest articol doar în scopul demonstrației). **\n",
    "\n",
    "## Dezavantaje ale utilizării Random Forest\n",
    "1. ** Un dezavantaj major al random forest -ului constă în complexitatea lor. Au necesitat resurse mult mai multe de calcul, datorită numărului mare de arbori de decizie alăturat. **\n",
    "2. ** Datorită complexității lor, ei necesită mult mai mult timp pentru a se antrena decât alți algoritmi comparabili. **\n",
    "\n",
    "### Bibliografie:\n",
    "1. https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "1. https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "1. https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** GradientBoosting construiește un model aditiv într-o etapă avansată; permite optimizarea funcțiilor de pierdere diferențiere arbitrare. În fiecare etapă, un arbore de regresie se încadrează pe gradientul negativ al funcției de pierdere date.\n",
    "Gradient Tree Boosting sau Gradient Boosted Tree (GBDT) este o generalizare a stimulării funcțiilor de pierdere arbitrare diferențiale. GBDT este o procedură precisă și eficientă în afara raftului, care poate fi folosită atât pentru probleme de regresie cât și de clasificare într-o varietate de domenii, inclusiv clasarea căutării web și ecologia. **\n",
    "\n",
    "** Ideea de stimulare a gradientului a luat naștere din observația de către Leo Breiman că stimularea poate fi interpretată ca un algoritm de optimizare pe o funcție de cost adecvată. Algoritmii de stimulare a gradientului de regresie explicită au fost ulterior dezvoltați de Jerome H. Friedman, simultan cu perspectiva mai generală de stimulare a gradientului funcțional de Llew Mason, Jonathan Baxter, Peter Bartlett și Marcus Frean. Ultimele două lucrări au introdus punctul de vedere al algoritmilor de stimulare ca algoritmi iterativi de coborâre a gradientului funcțional. Adică algoritmi care optimizează o funcție de cost peste spațiul funcțional prin alegerea iterativă a unei funcții (ipoteză slabă) care indică direcția negativă a gradientului. Această viziune gradientă funcțională a impulsului a dus la dezvoltarea algoritmilor de impuls în multe domenii ale învățării automate și a statisticilor dincolo de regresie și clasificare. **\n",
    "\n",
    "![RandomForestRegressor Ex1](./images/GBDTEx1-250x250.png)\n",
    "\n",
    "## Cum funcționează stimularea gradientului?\n",
    "### Creșterea gradientului implică trei elemente:\n",
    "1. ** O funcție de pierdere care trebuie optimizată. **\n",
    "1. ** Un elev slab să facă predicții. **\n",
    "1. ** Un model aditiv pentru a adăuga studenți slabi pentru a reduce la minimum funcția de pierdere. **\n",
    "\n",
    "## Avantajele ale Gradient Boosting\n",
    "1. ** Precizie mai bună: Regresia de stimulare a gradientului oferă, în general, o precizie mai bună. Când comparăm precizia GBR cu alte tehnici de regresie, cum ar fi Linear Regress, GBR este cel mai câștigător tot timpul. Acesta este motivul pentru care GBR este utilizat în majoritatea hackathon-urilor și competițiilor online. **\n",
    "1. ** Mai puțin pre-procesare: după cum știm că pre-procesarea datelor este unul dintre pașii vitali în procesul de învățare a mașinii, iar dacă nu o facem corect atunci afectează exactitatea modelului nostru. Cu toate acestea, Gradient Boosting Regression necesită o preprocesare minimă a datelor, ceea ce ne ajută să implementăm mai rapid acest model cu o complexitate mai mică. Deși pre-procesarea nu este obligatorie aici, ar trebui să reținem că putem îmbunătăți performanța modelului, petrecând timp în pre-procesarea datelor. **\n",
    "1. ** Flexibilitate mai mare: Regresia de gradient care oferă regresie poate fi folosită cu multe funcții de hipermetru și pierdere. Acest lucru face modelul extrem de flexibil și poate fi utilizat pentru a rezolva o mare varietate de probleme. **\n",
    "\n",
    "### Bibliografie:\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "1. https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\n",
    "1. https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "1. https://blog.paperspace.com/implementing-gradient-boosting-regression-python/\n",
    "1. https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}