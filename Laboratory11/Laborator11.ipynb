{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folosind un set de date - de exemplu de la https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=taskDown&view=table - sa se rezolve o problema de clasificare sau regresie, plecand de la intrari de tip text.\n",
    "\n",
    "Se poate opta pentru codificare BOW, n-grams, word2vec sau altele adecvate. Modelele de predictie pot fi din biblioteca scikit-learn. Puteti folosi pentru preprocesare biblioteca [NLTK](https://www.nltk.org) etc.\n",
    "\n",
    "Pentru clasificare se va optimiza scorul F1; se vor raporta scorurile F1 si acuratetea. Pentru regresie se va optimiza scorul mean squared error; se vor raporta scorurile MSE, mean absolute error, r2.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "1. [Clasificare de SMS-uri](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "1. [Sentence Classification Data Set](https://archive.ics.uci.edu/ml/datasets/Sentence+Classification#)\n",
    "1. [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n",
    "1. [Victorian Era Authorship Attribution Data Set](https://archive.ics.uci.edu/ml/datasets/Victorian+Era+Authorship+Attribution)\n",
    "1. [Amazon Commerce reviews set Data Set](https://archive.ics.uci.edu/ml/datasets/Amazon+Commerce+reviews+set)\n",
    "1. [Farm Ads Data Set](https://archive.ics.uci.edu/ml/datasets/Farm+Ads)\n",
    "1. etc...\n",
    "\n",
    "\n",
    "Se vor investiga minim 2 seturi de date si pentru fiecare din ele minim 4 modele de clasificare sau regresie. Daca setul de date e deja impartit in train si test, se va folosi ca atare - setul de antrenare se va imparti, suplimentar in train + validation; altfel, se va face kfold CV, k=5. Valorile optimale ale hiperparametrilor vor fi alese prin random search si grid search.\n",
    "\n",
    "Pentru fiecare set de date:\n",
    "\n",
    "1. (2 x 0.5 p) Se descrie setul de date, in limba romana (continut, provenienta, problema etc.)\n",
    "1. (2 x 1 p) Se face analiza exploratorie, folosind cod Python: distributia claselor sau a valorilor continue de iesire - numeric si grafic, statistici asupra textelor (de exemplu: lungime minima/medie/maxima; cele mai frecvente k cuvinte; clustering etc.). Se va explica fiecare pas si ce se urmareste prin efectuarea lui. Graficele vor avea axele numite (ce se reprezinta, evetual unitate de masura)\n",
    "1. (2 x 0.5 p) Se face preprocesare de date; se explica in limba romana care sunt metodele de preprocesare folosite, efectul lor pe datele de intrare, ce forma are iesirea obtinuta; se arata efectele pasilor de preprocesare asupra setului de date (noul numar de documente, dinamica vocabularului, trasaturile rezultate etc.) Se pot aduga grafice si tabele la acest pas.\n",
    "1. (2 x 4 x 0.5 p) Clasificare sau regresie, dupa caz: se face o descriere a modelelor considerate, in limba romana; se descrie modalitatea de cautare a hiperparametrilor; rezultatele obtinute se vor prezenta tabelar, similar cu tema precedenta. \n",
    "\n",
    "Se acorda doua puncte din oficiu.\n",
    "\n",
    "Descrierea modelelor si a pasilor de preprocesare pot fi in sectiuni separate, cu referinte la acestea unde e necesar. Partea specifica aplicarii pasilor pe datele considerate va fi prezentata respectand ordinea de aplicare. \n",
    "\n",
    "Exemple:\n",
    "\n",
    "1. [Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "1. [Text Classification with Python and Scikit-Learn](https://stackabuse.com/text-classification-with-python-and-scikit-learn/)\n",
    "1. [How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "\n",
    "Predarea temei se va face prin platforma de elearning in saptamana 25-29 mai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrierea setului de date Sentiment Labelled Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scorul este 1 (pentru pozitiv) sau 0 (pentru negativ)\n",
    "\n",
    "Pentru fiecare site web, există 500 de propoziții pozitive și 500 de propoziții negative. Acestea au fost selectate la întâmplare pentru seturi de date mai mari de recenzii.\n",
    "Am încercat să selectăm propoziții care au o conotație clar pozitivă sau negativă, obiectivul fiind ca nici o propoziție neutră să nu fie selectată.\n",
    "\n",
    "### Exemplu de continut:\n",
    "\n",
    "Wow... Loved this place.\t1\n",
    "\n",
    "Crust is not good.\t0\n",
    "\n",
    "Not tasty and the texture was just nasty.\t0\n",
    "\n",
    "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \t0\n",
    "\n",
    "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \t0\n",
    "\n",
    "Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \t0\n",
    "\n",
    "Very little music or anything to speak of.  \t0\n",
    "\n",
    "The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  \t1\n",
    "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\n",
    "\n",
    "Good case, Excellent value.\t1\n",
    "\n",
    "Great for the jawbone.\t1\n",
    "\n",
    "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\n",
    "\n",
    "The mic is great.\t1\n",
    "\n",
    "### Problema este una de clasificare binara: mesaj pozitiv = 1 si mesaj negativ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrierea setului de date SMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colecția SMS Spam v.1 (denumită în continuare corpus) este un set de mesaje marcate prin SMS care au fost colectate pentru cercetarea SMS Spam.\n",
    "\n",
    "Conține un set de mesaje SMS în limba engleză de 5.574 de mesaje, conform căruia etichetarea este ham (legitimă) sau spam.\n",
    "\n",
    "### Exemplu de continut:\n",
    "\n",
    "0   What you doing?how are you?\n",
    "\n",
    "0   Ok lar... Joking wif u oni...\n",
    "\n",
    "\n",
    "0   dun say so early hor... U c already then say...\n",
    "\n",
    "0   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
    "\n",
    "0   Siva is in hostel aha:-.\n",
    "\n",
    "0   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
    "\n",
    "1   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
    "\n",
    "1   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
    "\n",
    "1   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
    "\n",
    "### Problema este una de clasificare binara: mesaj acceptat = 0 si marcate ca spam = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primul model de clasificare DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arborii de decizie sunt o metodă de învățare supravegheată non-parametrică folosită pentru clasificare și regresie. Scopul este de a crea un model care prezice valoarea unei variabile țintă, învățând reguli simple de decizie deduse din caracteristicile datelor.**\n",
    "\n",
    "**Avantaje ale arborilor de decizie:**\n",
    "- Simplu de înțeles și de interpretat. Arborii pot fi vizualizați.\n",
    "- Necesită puțină pregătire a datelor. Alte tehnici necesită adesea normalizarea datelor, trebuie create variabile manechin și eliminarea valorilor necompletate. De reținut însă că acest modul nu acceptă valori lipsă.\n",
    "- Costul utilizării arborelui (adică, prezicerea datelor) este logaritmic în numărul de puncte de date utilizate pentru antrenarea arborelui.\n",
    "- Capabil să se ocupe de date atât numerice cât și categorice. Alte tehnici sunt de obicei specializate în analiza seturilor de date care au un singur tip de variabilă.\n",
    "- Capabil să gestioneze problemele cu ieșire multiplă.\n",
    "- Utilizează un model de 'cutie albă'. Dacă o anumită situație este observabilă într-un model, explicația pentru această condiție este ușor explicată prin logica booleană. În schimb, într-un model de cutie neagră (de exemplu, într-o rețea neuronală artificială), rezultatele pot fi mai dificil de interpretat.\n",
    "- Este posibil să se valideze un model folosind teste statistice. Acest lucru face posibilă contabilizarea fiabilității modelului.\n",
    "- Funcționează bine chiar dacă presupunerile sale sunt oarecum încălcate de modelul adevărat din care au fost generate datele.\n",
    "\n",
    "**Dezavantajele arborilor de decizie:**\n",
    "- Cei care învață arbori de decizie pot crea arbori supra-complecși care nu generalizează bine datele. Aceasta se numește supra-montare. Pentru a evita această problemă, sunt necesare mecanisme precum tăierea, stabilirea numărului minim de eșantioane necesare la un nod de frunze sau setarea adâncimii maxime a arborelui.\n",
    "- Arborii de decizii pot fi instabili, deoarece mici variații ale datelor pot duce la generarea unui arbore complet diferit. Această problemă este atenuată folosind arbori de decizie în cadrul unui ansamblu.\n",
    "- Problema învățării unui arbore decizional optim este cunoscută a fi NP-completă sub mai multe aspecte ale optimității și chiar pentru concepte simple. În consecință, algoritmii de învățare practică pe arbori de decizie se bazează pe algoritmi euristici, cum ar fi algoritmul lacom în care se iau decizii locale optime la fiecare nod. Astfel de algoritmi nu pot garanta returnarea arborelui decizional optim. Acest lucru poate fi atenuat prin antrenarea mai multor copaci într-un cursant de ansamblu, unde caracteristicile și eșantioanele sunt eșantionate aleatoriu cu înlocuire.\n",
    "- Există concepte greu de învățat, deoarece arborii de decizie nu le exprimă ușor, cum ar fi XOR, probleme de paritate sau multiplexor.\n",
    "- Cei care învață arborii decizionali creează arbori părtinitori dacă domină anumite clase. Prin urmare, se recomandă echilibrarea setului de date înainte de a se potrivi cu arborele de decizie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al doilea model de clasificare KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despre: \n",
    "Clasificarea bazată pe KNeighborsClassifier este un tip de învățare bazată pe instanțe care nu generalizează: nu încearcă să construiască un model intern general, ci pur și simplu stochează cazuri de instruire. Clasificarea se calculează pe baza votului cu majoritate simplă a vecinilor apropiați din fiecare punct: un semn de întrebare este atribuit clasei de date care are cei mai mulți reprezentanți din vecinii apropiați ai acestui punct.\n",
    "\n",
    "Scikit-learning implementează două clasificatoare diferite de vecini: KNeighborsClassifier implementează învățarea bazată pe\n",
    "vecinii cei mai apropiați ai fiecărui punct de interogare, unde este o valoare integrală specificată de utilizator. RadiusNeighborsClassifier implementează învățarea în funcție de numărul de vecini pe o rază fixă ​​a fiecărui punct de antrenament, unde este o valoare în virgulă flotantă specificată de utilizator.\n",
    "\n",
    "Clasificarea vecinătății în KNeighborsClassifier este cea mai frecvent utilizată tehnică. Alegerea optimă a valorii depinde în mare măsură de date: în general una mai mare suprima efectele zgomotului, dar face ca limitele de clasificare să fie mai puțin distincte.\n",
    "\n",
    "### Parametrii:\n",
    "- n_neighbors : int, optional (default = 5) - Numărul de vecini care vor fi folosiți în mod implicit.\n",
    "- weights : str, optional (default = ‘uniform’) - funcția de greutate folosită în predicție. Valori posibile: „Uniform”, „Distanță”,  callable.\n",
    "- algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional - Algoritmul folosit pentru calcularea vecinilor apropiați.\n",
    "- leaf_size : int, optional (default = 30) - Dimensiunea frunzelor a trecut la BallTree sau KDTree. Acest lucru poate afecta viteza de construcție și interogare, precum și memoria necesară pentru a păstra arborele. Valoarea optimă depinde de natura problemei.\n",
    "- p : integer, optional (default = 2) - M distanței de utilizat pentru arbore. Metrica implicită este minkowski, iar cu p = 2 este echivalentă cu metrica euclidiană standard.\n",
    "- metric : string, default ‘minkowski’ - Pe măsură ce leaf_size crește, memoria necesară pentru a stoca o structură de arbore scade. Acest lucru este important mai ales în cazul arborelui cu bilă, care stochează un centroid dimensional pentru fiecare nod.\n",
    "- metric_params : dict, optional (default = None) - Argumente suplimentare de cuvinte cheie pentru funcția metrică.\n",
    "- n_jobs : int or None, optional (default=None) - Numărul de locuri de muncă paralele care trebuie executate pentru căutarea vecinilor.\n",
    "\n",
    "### Atribute:\n",
    "- classes_ : array of shape (n_classes,) - Etichete de clasă cunoscute de clasificator.\n",
    "- effective_metric_ : string or callble - Măsura distanței utilizate. Acesta va fi același cu parametrul metric sau un sinonim cu acesta, de ex. „Euclidian” dacă parametrul metric este setat pe „minkowski” și parametrul p setat pe 2.\n",
    "- effective_metric_params_dict - Argumente suplimentare de cuvinte cheie pentru funcția metrică. Pentru majoritatea valorilor metrice vor fi aceleași cu parametrul metric_params, dar poate conține și valoarea parametrului p dacă atributul effective_metric_ este setat pe „minkowski”.\n",
    "- outputs_2d_bool - Fals atunci când forma y este (n_samples,) sau (n_samples, 1) în timpul ajustării altfel True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al treilea model de clasificare Support vector machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVM) sunt un set de metode de învățare supravegheate utilizate pentru clasificare, regresie și detectarea valorilor exterioare.\n",
    "\n",
    "Implementarea se bazează pe libsvm. Timpul de încadrare se scalează cel puțin în mod quadrat cu numărul de probe și poate fi nepractic dincolo de zeci de mii de probe. Pentru seturi de date mari, luați în considerare utilizarea în schimb, sklearn.svm.LinearSVC sau sklearn.linear_model.SGDClassifier, eventual după un transformator sklearn.kernel_approximation.Nystroem.\n",
    "\n",
    "### Avantajele mașinilor de susținere sunt:\n",
    "- Eficient în spații cu dimensiuni mari.\n",
    "- Încă este eficient în cazurile în care numărul de dimensiuni este mai mare decât numărul de eșantioane.\n",
    "- Utilizează un subset de puncte de instruire în funcția decizională (numiți vectori de susținere), deci este, de asemenea, eficient în memorie.\n",
    "- Versatil: diferite funcții Kernel pot fi specificate pentru funcția de decizie. Sunt furnizate sâmburele comune, dar este de asemenea posibil să se specifice nuclee personalizate.\n",
    "\n",
    "### Dezavantajele mașinilor vectoriale de suport includ:\n",
    "- Dacă numărul de caracteristici este mult mai mare decât numărul de eșantioane, evitați montarea excesivă în alegerea funcțiilor Kernel, iar termenul de regularizare este crucial.\n",
    "- SVM-urile nu furnizează în mod direct estimări ale probabilității, acestea sunt calculate folosind o validare încrucișată de cinci ori mai scumpă (vezi Scoruri și probabilități, mai jos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al cincilea model de clasificare RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un random forest este un meta-estimator care se potrivește cu mai multe clasificatoare de arbori de decizie pe diverse sub-eșantioane ale setului de date și folosește medie pentru a îmbunătăți exactitatea predictivă și pentru a controla supra-montarea. Mărimea sub-eșantionului este întotdeauna aceeași cu dimensiunea inițială a eșantionului de intrare, dar eșantioanele sunt trase cu înlocuire dacă bootstrap = True (implicit).\n",
    "\n",
    "În pădurile aleatorii, fiecare copac din ansamblu este construit dintr-un eșantion extras cu înlocuire (adică, un eșantion de bootstrap) din setul de antrenament.\n",
    "\n",
    "Mai mult, atunci când se divid, fiecare nod în timpul construcției unui arbore, cea mai bună împărțire se găsește fie din toate caracteristicile de intrare, fie dintr-un subset aleatoriu de dimensiuni max_features.\n",
    "\n",
    "Scopul acestor două surse de aleatoriu este reducerea variației estimatorului de forest. Într-adevăr, arborii de decizie individuali prezintă, de obicei, o variație mare și tind să se împerecheze. Randomul injectat din păduri produce arbori decizionali cu erori de predicție oarecum necuplate. Luând în medie aceste predicții, unele erori pot fi anulate. Pădurile aleatorii obțin o variație redusă prin combinarea unor arbori diversi, uneori cu prețul unei ușoare creșteri a părtinirilor. În practică, reducerea variației este adesea semnificativă, rezultând deci un model global mai bun.\n",
    "\n",
    "Valorile implicite pentru parametrii care controlează dimensiunea arborilor (de exemplu, max_depth, min_samples_leaf, etc.) duc la arbori în totalitate crescuți, care pot fi foarte mari pe unele seturi de date. Pentru a reduce consumul de memorie, complexitatea și dimensiunea arborilor ar trebui controlate aceste valori ale parametrilor.\n",
    "\n",
    "Funcțiile sunt permise întotdeauna la întâmplare la fiecare împărțire. Prin urmare, cel mai bine divizat poate varia, chiar și cu aceleași date de instruire, max_features = n_features și bootstrap = False, dacă îmbunătățirea criteriului este identică pentru mai multe divizii enumerate în timpul căutării celei mai bune împărțiri. Pentru a obține un comportament determinist în timpul montării, random_state trebuie să fie stabilit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategia de cautare GirdSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Căutare exhaustivă a hiperparametrilor optimi peste valorile parametrilor specificate pentru un estimator. \n",
    "\n",
    "Membrii importanți sunt `fit`, `predict`.\n",
    "\n",
    "GridSearchCV implementează o metodă `fit` și `score`. De asemenea, implementează `predict`, `predict_proba`, `decision_function`, `transform` și `invers_transform` dacă sunt implementate în estimatorul utilizat.\n",
    "\n",
    "Parametrii estimatorului utilizați pentru aplicarea acestor metode sunt optimizați prin căutare de grilă validată încrucișat peste o grilă de parametri.\n",
    "\n",
    "Parametrii selectați sunt cei care maximizează scorul datelor rămase, cu excepția cazului în care un punctaj explicit este trecut în acest caz, în schimb este utilizat.\n",
    "\n",
    "Dacă `n_jobs` a fost setat la o valoare mai mare decât 1, datele sunt copiate pentru fiecare punct din grilă (și nu `n_jobs` ori). Acest lucru se realizează din motive de eficiență dacă lucrările individuale durează foarte puțin timp, dar pot ridica erori dacă setul de date este mare și nu este disponibilă suficientă memorie. O soluție de rezolvare în acest caz este să setați `pre_dispatch`. Apoi, memoria este copiată doar `pre_dispatch` de mai multe ori. O valoare rezonabilă pentru `pre_dispatch` este `2*n_jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategia de cautare RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Căutare aleatorie a hiperparametrilor.\n",
    "\n",
    "RandomizedSearchCV implementează o metodă `fit` și `score`. De asemenea, implementează `predict`, `predict_proba`, `decision_function`, `transform` și `invers_transform` dacă sunt implementate în estimatorul utilizat.\n",
    "\n",
    "Parametrii estimatorului utilizați pentru aplicarea acestor metode sunt optimizați prin căutare validată încrucișată peste setările parametrilor.\n",
    "\n",
    "Spre deosebire de GridSearchCV, nu toate valorile parametrilor sunt încercate, ci mai degrabă un număr fix de parametri este prelevat din distribuțiile specificate. Numărul de parametri încercați este dat de n_iter.\n",
    "\n",
    "Dacă toți parametrii sunt prezenți ca o listă, se efectuează eșantionarea fără înlocuire. Dacă cel puțin un parametru este dat ca distribuție, se folosește eșantionarea cu înlocuire. Este foarte recomandat să folosiți distribuții continue pentru parametri continuu.\n",
    "\n",
    "Parametrii selectați sunt cei care maximizează scorul datelor păstrate, conform parametrului de notare.\n",
    "\n",
    "Dacă `n_jobs` a fost setat la o valoare mai mare decât 1, datele sunt copiate pentru fiecare setare a parametrilor (și nu `n_jobs` ori). Acest lucru se realizează din motive de eficiență dacă lucrările individuale durează foarte puțin timp, dar pot ridica erori dacă setul de date este mare și nu este disponibilă suficientă memorie. O soluție de rezolvare în acest caz este să setați `pre_dispatch`. Apoi, memoria este copiată doar `pre_dispatch` de ori. O valoare rezonabilă pentru `pre_dispatch` este `2*n_jobs`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}