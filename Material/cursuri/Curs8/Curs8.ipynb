{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curs 8. Lucrul cu date de tip text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: prezentarea si codul sunt conform [Introduction to Machine Learning with Python: A Guide for Data Scientists](https://www.bookdepository.com/Introduction-Machine-Learning-with-Python-Andreas-C-Mueller-Sarah-Guido/9781449369415)\n",
    "\n",
    "O clasa aparte de probleme este cea data de procesarea de informatie de tip text. Exemple de probleme:\n",
    "* clasificarea mailului ca fiind de tip spam sau legitim\n",
    "* analiza sentimentelor pe baza elementelor scrise\n",
    "* regasirea de texte similare\n",
    "\n",
    "Exista urmatoarele 4 categorii de date text:\n",
    "1. date categoriale\n",
    "1. text liber reductibil la date categoriale\n",
    "1. text structurat\n",
    "1. text liber\n",
    "\n",
    "**Datele categoriale** provin dintr-o lista predefinita (ex: genul unei persoane, culoarea unei masini):\n",
    "* \"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"purple\", \"pink\"; \"multicolo(u)red\", \"other\"\n",
    "* \"male\", \"female\", \"not declared\". \n",
    "\n",
    "Totusi, in functie de modul de prelaure a valorii de la utilizator, este inca posibil sa apara erori de scriere: blak in loc de black, sau particularitati de scriere: gray/grey, neighbor/neighbour. Se impune deci o determinare a formelor corecte, de exemplu prin [distanta Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) sau [algoritmi de spell-checking](https://norvig.com/spell-correct.html) - atentie la dialectul ales. In alte cazuri este nevoie de reducerea dictionarului, pentru notiuni care sunt foarte similare sau identice: \n",
    "* \"culoarea ierbii\" -> \"verde\", \n",
    "* definirea unor categorii de ocupatii iar la final \"Altele\"\n",
    "* [Colours](http://www.thedoghousediaries.com/1406)\n",
    "\n",
    "In alte cazuri, exprimari libere (texte de dimensiuni medii) pot fi reduse la **categorii cunoscute**. Pentru simplificarea procesarii, se recomanda evitarea de introducere de text liber si sa se permita alegerea dintr-o multime predefinita de valori:\n",
    "\n",
    "<img src=\"./images/dropdown.gif\" alt=\"Dropdown\" width=\"200px\"/>\n",
    "\n",
    "**Textul structurat** poate fi exemplificat prin documente XML sau fisiere JSON, care respecta o anumita sintaxa si eventual o schema. De exemplu, datele de contact au o structura (adresa fizica, persoana de contact, telefon), interpretarea lor facandu-se cu cunostinte de domeniu. Procesarea lor este un subiect separat. \n",
    "\n",
    "Ultima categorie - **textul liber** - este data de documente de intindere mai mare: email, carti, tweets, scrisori de intentie etc. Procesarea lor intra in zona Natural Language Processing (NLP) si information retrieval (IR). Un exemplu este  determinarea plagiatelor, prin care se detecteaza preluari masive din alte surse text, sau atribuirea autorului - pentru o bucata de text se cere determinarea autorului cel mai probabil; [Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution](https://arxiv.org/abs/1609.06686); detectarea autorului unui cod compilat [Even Anonymous Coders Leave Fingerprints](https://www.wired.com/story/machine-learning-identify-anonymous-code/), articol la [When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries](https://arxiv.org/pdf/1512.08546.pdf).\n",
    "\n",
    "Sursele de date sunt diverse: \n",
    "* pagini Wikipedia\n",
    "* cartile din [proiectul Gutenberg](http://www.gutenberg.org/), la ora (aprilie 2020) peste 60000 de ebooks\n",
    "* [mesaje newgroups](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html)\n",
    "* [ziare](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)\n",
    "* [Alphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP)](https://github.com/niderhoff/nlp-datasets)\n",
    "* [IMDB Movie Review Sentiment Classification (stanford)](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "* [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\n",
    "* [Resursa 1](https://stats.stackexchange.com/questions/18905/where-to-find-a-large-text-corpus), [Colectia 2](https://lionbridge.ai/datasets/the-best-25-datasets-for-natural-language-processing/), [Colectia 3](https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplu de aplicatie: analiza sentimentor din recenzii de filme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un set de recenzii de filme facute de catre utilizator, disponibil [aici](http://ai.stanford.edu/~amaas/data/sentiment/). Pentru fiecare recenzie exista o valoare numerica intre 1 si 10, reprezentand o indicatie: daca este un review pozitiv sau negativ. Setul de date este impartit in subset de antrenare si de testare, iar pentru fiecare subset se gasesc doua directoare, respectiv pentru aprecieri pozitive (rating > 5) si negative (rating <= 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:24:54.007669Z",
     "start_time": "2020-04-14T06:24:53.945806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is system\n",
      " Volume Serial Number is DEBB-6518\n",
      "\n",
      " Directory of c:\\temp\\Curs8\\data\\aclImdb\n",
      "\n",
      "04/13/2020  07:51 PM    <DIR>          .\n",
      "04/13/2020  07:51 PM    <DIR>          ..\n",
      "04/12/2011  08:14 PM           845,980 imdb.vocab\n",
      "06/12/2011  01:54 AM           903,029 imdbEr.txt\n",
      "06/26/2011  03:18 AM             4,037 README\n",
      "04/13/2020  07:49 PM    <DIR>          test\n",
      "04/13/2020  07:51 PM    <DIR>          train\n",
      "               3 File(s)      1,753,046 bytes\n",
      "               4 Dir(s)  64,399,880,192 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:24:55.343000Z",
     "start_time": "2020-04-14T06:24:54.481235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder PATH listing for volume system\n",
      "Volume serial number is DEBB-6518\n",
      "C:\\TEMP\\CURS8\\DATA\\ACLIMDB\n",
      "+---test\n",
      "|   +---neg\n",
      "|   \\---pos\n",
      "\\---train\n",
      "    +---neg\n",
      "    +---pos\n",
      "    \\---unsup\n"
     ]
    }
   ],
   "source": [
    "!tree \"data/aclImdb\" /A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:24:55.362988Z",
     "start_time": "2020-04-14T06:24:55.355964Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = './data/aclImdb/'\n",
    "path_train = os.path.join(path, 'train')\n",
    "path_test = os.path.join(path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:24:57.632055Z",
     "start_time": "2020-04-14T06:24:55.519026Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:25:46.615099Z",
     "start_time": "2020-04-14T06:24:57.649056Z"
    }
   },
   "outputs": [],
   "source": [
    "####### !!!16 mins running time!!!\n",
    "reviews_train = load_files(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:25:47.836006Z",
     "start_time": "2020-04-14T06:25:47.827031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 75000\n",
      "A text:  b\"Having watched all of the Star Trek TV series episodes many times each since the 1960s, most being quite good to superb, and only very few being mediocre, my opinion is that this one is the worst of all.<br /><br />In fact, I think it's so poorly executed as to be an embarrassment to the series. It's not that the story is so bad, although it's not particularly outstanding in any way, but the acting is just abysmal on the part of the two lead characters, meaning those other than the regulars in this case. Barbara Anderson gives her weakest performance ever as the daughter of a mass killer, and who is on a mission of a sort. She practically calls in the role from a phone, and shows no real emotive abilities here. Although usually she's never used as more than a pretty face in most of her film/TV roles,usually small parts, she has done much better.<br /><br />Arnold Moss as her father gives new meaning to the term 'Ham' and is the only actor ever on a 1960s Star Trek episode that outdid William Shatner in this area, and actually makes Shatner look superb by comparison. And he gets to play a Shakespearian actor no less, which gives him more impetus to overact, and he does so.<br /><br />Other than these two leads being so weak, the story is such that anybody with any sense at all can tell who the killer is within the first 15 minutes. I say this because I told my brother the whole plot ending at the first commercial break when we were watching the original 1966 broadcast as pre-teens. His reply was, Yeah, you're right.<br /><br />Skip this one and watch the much superior Menagerie episodes which were originally televised right before.\"\n",
      "Associated review: 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print('How are texts organized: ', type(text_train))\n",
    "print('How many texts in train subset', len(text_train))\n",
    "print('A text: ', text_train[50100])\n",
    "print('Associated review:', y_train[50100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:05.809011Z",
     "start_time": "2020-04-14T06:25:50.159313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_test = load_files(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:08.606021Z",
     "start_time": "2020-04-14T06:26:08.576966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are texts organized:  <class 'list'>\n",
      "How many texts in train subset 25000\n",
      "The first text:  b'A great gangster flick, with brilliant performances by well-known actors with great action scenes? Well, not this one.<br /><br />It\\'s rather amazing to see such a wide cast of well-known actors, that have many good movies in their filmographies in such a movie, without doubt this may be one of the worst they could possibly appear in.<br /><br />First of all, the plot is as you\\'d expect it from your average gangster biography, nothing new, nothing fancy in it. The way it is told makes the movie look a LOT longer than it is (when i thought the two hours should be almost over, i was quite surprised that only 45 minutes had passed).<br /><br />The action scenes look a lot like those from 80ies TV series - the A-Team, for example. It\\'s just that in the 80ies (esp. with the A-Team) those scenes were far more sophisticated than those in \"El Padrino\". It\\'s especially fun to see the guys point their guns in the air and still hit something (not to talk about people that take cover behind car doors which later look like they\\'ve been shot through).<br /><br />The acting fits quite nicely to the action. Either you get the same reaction to everything that happens (Dolph Lundgren style), or it\\'s so overacted that you may think it\\'s a parody (but unfortunately it\\'s not).<br /><br />My advise is to stay away from this movie, any other gangster movie is better than this one.'\n",
      "Associated review: 0\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print('How are texts organized: ', type(text_test))\n",
    "print('How many texts in train subset', len(text_test))\n",
    "print('The first text: ', text_test[70])\n",
    "print('Associated review:', y_test[70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remarca existenta elementului html `<br />` ce poate fi inlaturat, fara a afecta continutul mesajului:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:11.915023Z",
     "start_time": "2020-04-14T06:26:11.501168Z"
    }
   },
   "outputs": [],
   "source": [
    "text_train = [text.replace(b'<br />', b' ') for text in text_train]\n",
    "text_test = [text.replace(b'<br />', b' ') for text in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numarul de elemente din fiecare clasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:14.919543Z",
     "start_time": "2020-04-14T06:26:14.883052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in train set:  [0 1 2]\n",
      "Classes in test set:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Classes in train set: ', np.unique(y_train))\n",
    "print('Classes in test set: ', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:18.209306Z",
     "start_time": "2020-04-14T06:26:18.048736Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filtering out items of class \"2\" in train set\n",
    "text_train = [text_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]\n",
    "y_train = [y_train[i] for i in range(len(y_train)) if y_train[i] < 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:20.478973Z",
     "start_time": "2020-04-14T06:26:20.456990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class in training set: [12500 12500]\n",
      "Samples per class in test set: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print('Samples per class in training set: {0}'.format(np.bincount(y_train)))\n",
    "print('Samples per class in test set: {0}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentarea textului sub forma bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru un text, reprezenatrea bag of word se obtine astfel: se pleaca de la un vocabular  = multimea tuturor cuvintelor care pot aparea in texte - si se contorizeaza pentru fiecare cuvant din dictionar de cat ori apare in text. Daca vocabularul are $k$ cuvinte distincte, atunci pentru fiecare document rezulta un vector de $k$ componente. Majoritatea componentelor din vectorul asociat unui document va fi zero, deci avem de-a face cu vectori rari. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasii pentru obtinerea unei reprezentari bag of words sunt:\n",
    "1. despartirea in cuvinte (tokenizing)\n",
    "1. construirea dictionarului\n",
    "1. construirea documentului bag of word pentru document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bow_processing.png' width='600px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplu de obtinere de bag of words pe un text simplu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:22.691026Z",
     "start_time": "2020-04-14T06:26:22.682990Z"
    }
   },
   "outputs": [],
   "source": [
    "bards_words =[\n",
    "    \"The fool doth think he is wise,\",\n",
    "    \"but the wise man knows himself to be fool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasa `CountVectorizer` din `sklearn.feature_extraction.text` serveste la selectarea cuvintelor din text si calculul frecventei de aparitie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:25.248992Z",
     "start_time": "2020-04-14T06:26:25.213009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n",
      "[('be', 0), ('but', 1), ('doth', 2), ('fool', 3), ('he', 4), ('himself', 5), ('is', 6), ('knows', 7), ('man', 8), ('the', 9), ('think', 10), ('to', 11), ('wise', 12)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(vect.vocabulary_) # cuvintele distincte din texte\n",
    "\n",
    "# vocabular sortat in ordine crescatoare\n",
    "print(sorted(vect.vocabulary_.items(), key = lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtinerea unui vector bag of words se obtine prin aplicarea metodei `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:27.879992Z",
     "start_time": "2020-04-14T06:26:27.874009Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = vect.transform(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:30.269953Z",
     "start_time": "2020-04-14T06:26:30.255991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprezentarea ca vectori rari\n",
      ":\b  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "Reprezentarea ca vectori:\n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Reprezentarea ca vectori rari\\n:\\b{bow}')\n",
    "print('Reprezentarea ca vectori:\\n', bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformarea celor doua subseturi de date in BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:56.810582Z",
     "start_time": "2020-04-14T06:26:32.736036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(text_train)\n",
    "print(repr(X_train))\n",
    "X_test = vect.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:26:59.667049Z",
     "start_time": "2020-04-14T06:26:59.659026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular: 74849\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensiune vocabular: {len(vect.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:02.336514Z",
     "start_time": "2020-04-14T06:27:02.201875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiunea vocabularului: 74849\n"
     ]
    }
   ],
   "source": [
    "#obtinerea vocabularului\n",
    "feature_names = vect.get_feature_names()\n",
    "print('Dimensiunea vocabularului:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:04.883979Z",
     "start_time": "2020-04-14T06:27:04.870985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:07.331979Z",
     "start_time": "2020-04-14T06:27:07.321968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['är', 'ääliöt', 'äänekoski', 'åge', 'åmål', 'æsthetic', 'écran', 'élan', 'émigré', 'émigrés', 'était', 'état', 'étc', 'évery', 'êxtase', 'ís', 'ísnt', 'østbye', 'über', 'üvegtigris']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:09.747934Z",
     "start_time": "2020-04-14T06:27:09.737961Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draper', 'draperies', 'drapery', 'drapes', 'draskovic', 'drastic', 'drastically', 'drat', 'dratch', 'dratic', 'dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names[20000:20020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antrenarea si evaluarea unui model de logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:12.414955Z",
     "start_time": "2020-04-14T06:27:12.148109Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:30.813532Z",
     "start_time": "2020-04-14T06:27:14.843299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea medie de clasificare pe setul de instruire: 0.88272\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv = 5, n_jobs=4)\n",
    "print('Acuratetea medie de clasificare pe setul de instruire: {0}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicare de cross validation pentru parametrul de regularizare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista un parametru de regularizare care determina penalizarea $L_2$ a ponderilor modelului. Efectuam cautarea valorii potrivite a acestui hiperparametru, al carui nume de argument este `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:27:33.487405Z",
     "start_time": "2020-04-14T06:27:33.477433Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:30:03.345232Z",
     "start_time": "2020-04-14T06:27:36.030170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8881599999999998\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# param_grid = {'C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "param_grid = {'C':[0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:30:06.006844Z",
     "start_time": "2020-04-14T06:30:05.953833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87884\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificarea parametrilor pentru obtinerea BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem cere retinerea doar a acelor cuvinte care apar intr-un numar minim de documente, de ex 5. In acest fel speram ca se elimina cuvintele care nu sunt larg partajate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:30:34.227846Z",
     "start_time": "2020-04-14T06:30:08.503828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  27271\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:32:22.279745Z",
     "start_time": "2020-04-14T06:30:37.052795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.88744\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:32:25.175843Z",
     "start_time": "2020-04-14T06:32:25.121473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8778\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminare cuvinte neinformative (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exista cuvinte care nu poarta multa informatie: prepozitii, conjunctii, sau cuvinte care se repeta des in toate documentele. Exista doua posibilitati de eliminare a acestora:\n",
    "1. Se foloseste o lista predefinita de stopwords, specifica limbii respective\n",
    "1. Se estimeaza valoarea informationala a cuvintelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru limba engleza sunt definite stopwords in sklearn.feature_extraction.text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:32:28.002033Z",
     "start_time": "2020-04-14T06:32:27.984081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:32:30.832228Z",
     "start_time": "2020-04-14T06:32:30.817271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'anywhere', 'must', 'seeming', 'across', 'their', 'about', 'whereafter', 'nevertheless', 'next', 'hers', 'seems', 'we', 'noone', 'itself', 'eight', 'are', 'above', 'be', 'cant']\n"
     ]
    }
   ],
   "source": [
    "print(list(ENGLISH_STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:32:54.960404Z",
     "start_time": "2020-04-14T06:32:33.641779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiune vocabular:  26966\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words='english')\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "print('Dimensiune vocabular: ', len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:33:22.045629Z",
     "start_time": "2020-04-14T06:32:56.987788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8837200000000001\n",
      "best params: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(X_train, y_train)\n",
    "print('best cross validation score:', grid.best_score_)\n",
    "print('best params:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:33:23.988828Z",
     "start_time": "2020-04-14T06:33:23.951917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87256\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculul valorii tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency - inverse document frequency da pondere unui cuvant care apare frecvent intr-un document, dar nu in multe documente - deci are putere descriptiva pentru documentul in care apare frecvent. Pentru un document $d$ si un cuvant $w$, valoarea tfidf se calculeaza ca:\n",
    "$$\n",
    "tfidf(w, d) = tf(w, d) \\cdot \\log\\left( \\frac{N+1}{N_w+1} \\right) + 1\n",
    "$$\n",
    "unde: $tf(w, d)$ este numarul de aparitii ale lui $w$ in document $d$, $N$ e numarul total de documente din corpus, $N_w$ este numarul de documente din corpus care contin cuvantul $w$. Pentru fiecare document $d$ se calculeaza un astfel de vector, care in final este normalizat in norma $L_2$ la 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:34:47.885364Z",
     "start_time": "2020-04-14T06:33:25.843165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.8886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm='l2'), \n",
    "                     LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "# param_grid = {'logisticregression__C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "param_grid = {'logisticregression__C':[0.01, 0.1, 1]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "grid.fit(text_train, y_train)\n",
    "print('Best cross validation score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:34:57.027106Z",
     "start_time": "2020-04-14T06:34:49.864989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizarea de n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOW pierde succesiunea cuvintelor. Se pot considera toate succesiunile de n cuvinte - n-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:34:58.949420Z",
     "start_time": "2020-04-14T06:34:58.936454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The fool doth think he is wise,', 'but the wise man knows himself to be fool']\n"
     ]
    }
   ],
   "source": [
    "print(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:35:00.883401Z",
     "start_time": "2020-04-14T06:35:00.863456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be fool',\n",
       " 'but the',\n",
       " 'doth think',\n",
       " 'fool doth',\n",
       " 'he is',\n",
       " 'himself to',\n",
       " 'is wise',\n",
       " 'knows himself',\n",
       " 'man knows',\n",
       " 'the fool',\n",
       " 'the wise',\n",
       " 'think he',\n",
       " 'to be',\n",
       " 'wise man']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T06:35:02.818066Z",
     "start_time": "2020-04-14T06:35:02.782164Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(solver='lbfgs', max_iter=1000))\n",
    "# param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "param_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T07:07:22.559253Z",
     "start_time": "2020-04-14T06:35:04.757066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidfvectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=5,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None...\n",
       "                                                           max_iter=1000,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=4,\n",
       "             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10],\n",
       "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 2),\n",
       "                                                          (1, 3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T07:07:34.042072Z",
     "start_time": "2020-04-14T07:07:34.009160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 10, 'tfidfvectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T07:08:19.881772Z",
     "start_time": "2020-04-14T07:07:37.349201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99928"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T07:09:05.080940Z",
     "start_time": "2020-04-14T07:08:22.422011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90364"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alte modalitati de procesare a textelor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec: [King - Man + Woman = Queen: The Marvelous Mathematics of Computational Linguistics](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/); [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec); [Understanding Word Embeddings](https://hackernoon.com/understanding-word-embeddings-a9ff830403ce)\n",
    "* [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
